# Assignment: Build a Creative Local LLM Agent Using Python 3.11

A local LLM Pokémon chatbot using Ollama,  `noahs_local_ollama_chat_agent`, and `noahs_tts` with a consolidated Pokémon knowledge base.

## Prerequisites
- macOS, zsh
- Ollama installed and a model pulled (e.g., `llama3.2:3b`)
- Python 3.11

## Setup
- Create and activate a virtual environment
```zsh
python3.11 -m venv .venv
```
- Activate the venv (zsh/macOS)
```zsh
source .venv/bin/activate
```

## Text-to-Speech (optional)
- Install the TTS dependency:
```zsh
pip install noahs_tts
```
- Run chat with spoken responses:
```zsh
python chat.py --tts
```
- Without TTS (default streaming output):
```zsh
python chat.py
```


- Install the dependency (with venv activated)
```zsh
pip install noahs-local-ollama-chat-agent
pip install noahs_tts
```

## Debug (optional)
- Enable semantic retrieval debug output:
```zsh
python chat.py --debug
```
- Combine with TTS:
```zsh
python chat.py --tts --debug
```

## Run

- Start Ollama separately
```zsh
ollama run llama3.2:3b  # or ensure the model is installed
```
- Chat demo (with activated venv)
```zsh
python chat.py
```

## Testing
Automated test runner generates a CSV report of semantic queries, questions, and Cynthia's responses.
```zsh
ollama run llama3.2:3b  # or ensure the model is installed
```        
- With venv activated
```zsh
python scripts/agent_test_runner.py
```
Output: `reports/agent_test_results.csv` with columns `test_id`, `semantic_query`, `question`, `response`.
The runner injects semantic context first (semantic query), then asks the question, and captures the full non-streamed response.

## Notes
- Use `--debug` in `chat.py` to show which KB chunks are retrieved.
- `.gitignore` excludes the local venv (`myenv/`) and DB/cache artifacts.
- The consolidated KB is at `data/processed/pokemon_kb.txt` and is regenerated by `preprocessing/build_kb.py`.
