# Assignment: Build a Creative Local LLM Agent Using Python 3.11

A local LLM Pokémon chatbot using Ollama,  `noahs_local_ollama_chat_agent`, and `noahs_tts` with a consolidated Pokémon knowledge base.

## Prerequisites
- macOS, zsh
- Ollama installed and a model pulled (e.g., `llama3.2:3b`)
- Python 3.11

## Setup
- Create and activate a virtual environment
```zsh
# Create a new venv (recommended)
python3.11 -m venv .venv
# Chat demo (with activated venv)
# Activate the venv (zsh/macOS)
source .venv/bin/activate

## Text-to-Speech (optional)
- Install the TTS dependency (once):
```zsh
pip install noahs_tts
```
- Run chat with spoken responses:
```zsh
python chat.py --tts
```
- Without TTS (default streaming output):
```zsh
python chat.py
```

```
- Install the dependency (with venv activated)
```zsh
pip install noahs-local-ollama-chat-agent
pip install noahs_tts
```

## Run
```zsh
# Start Ollama separately
ollama run llama3.2:3b  # or ensure the model is installed

# Chat demo (with activated venv)
python chat.py
```

## Testing
Automated test runner generates a CSV report of semantic queries, questions, and Cynthia's responses.
```zsh
ollama run llama3.2:3b  # or ensure the model is installed

# With venv activated
python scripts/agent_test_runner.py
```
Output: `reports/agent_test_results.csv` with columns `test_id`, `semantic_query`, `question`, `response`.
The runner injects semantic context first (semantic query), then asks the question, and captures the full non-streamed response.

## Notes
- Semantic debug is enabled in `chat.py` to show which chunks are retrieved.
- `.gitignore` excludes the local venv (`myenv/`) and DB/cache artifacts.
- The consolidated KB is at `data/processed/pokemon_kb.txt` and is regenerated by `preprocessing/build_kb.py`.
